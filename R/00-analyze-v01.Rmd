---
author: ""
date: ""
output:
  html_document::
    toc: false
---

```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(
  echo = TRUE,
  cache = FALSE,
  results = "markdown",
  fig.align = "center",
  fig.show = "asis",
  fig.width = 6,
  fig.height = 6,
  # out.width = 6,
  # out.height = 6,
  warning = FALSE,
  message = FALSE
)
```


```{r packages}
library("dplyr")
library("stringr")
library("gh")
library("purrr")
library("temisc")
library("tetext")
```

```{r define_params}
params <-
  list(
    color_main = "#7EC0EE"
  )
```

```{r import_posts_info, echo = FALSE}
# NOTE: Probably don't need to re-import `posts` for analything
# filepath_posts <- file.path("data", "posts-rweekly.rds")
# saveRDS(posts, filepath_posts)
# posts_info <- readRDS(filepath_posts)
filepath_posts_info <- file.path("data", "posts_info-rweekly.csv")
# readr::write_csv(posts_info, filepath_posts_info)
posts_info <- readr::read_csv(filepath_posts_info)
# names(posts_info) <- c("name", "path")
```

```{r posts_info, eval = FALSE}
# Rerference:
# + https://itsalocke.com/blog/markdown-based-web-analytics-rectangle-your-blog/
posts <-
  gh::gh(
    endpoint = "/repos/:owner/:repo/contents/:path",
    owner = "rweekly",
    repo = "rweekly.org",
    path = "_posts"
  )

# NOTE: Only do this to replicate the `posts` that were originally pulled.
# posts <- posts[1:93]

posts_info <-
  dplyr::data_frame(
    name = purrr::map_chr(posts, "name"),
    path = purrr::map_chr(posts, "path")
  )
```

```{r import_data, echo = FALSE}
filepath_data <- file.path("data", "data-rweekly.csv")
# readr::write_csv(data, filepath_data)
data <- readr::read_csv(filepath_data)
```

```{r get_rweekly_post_data, eval = FALSE}
get_rweekly_post_data <-
  function(filepath) {
    # NOTE: This would be necessary if downloading directly from the repo.
    # filepath <-
    #   gh::gh(
    #     "/repos/:owner/:repo/contents/:path",
    #     owner = "rweekly",
    #     repo = "rweekly.org",
    #     path = path
    #   )

    filepath_prefix <- "data-raw"
    filepath <- file.path(filepath_prefix, filepath)
    rgx_rmv <- "Å|â€|œ|\u009d"
    rgx_detect_link <- "^\\+\\s+\\["
    rgx_detect_head <- "^\\s*\\#"
    rgx_link_post <- "(?<=\\[).*(?=\\])"
    rgx_link_img <- "(?<=\\!\\[).*(?=\\])"
    rgx_url <- "(?<=\\().*(?=\\))"
    rgx_head <- "(?<=\\#\\s).*$"
    
    lines <- readLines(filepath)
    lines_proc <-
      lines %>%
      # base64enc::base64decode() %>%
      # rawToChar() %>%
      stringr::str_split("\n") %>%
      purrr::flatten_chr() %>%
      as_tibble() %>%
      rename(text = value) %>%
      transmute(line = row_number(), text) %>%
      filter(text != "") %>%
      mutate(text = stringr::str_replace_all(text, rgx_rmv, "")) %>%
      mutate(
        is_link = ifelse(stringr::str_detect(text, rgx_detect_link), TRUE, FALSE),
        is_head = ifelse(stringr::str_detect(text, rgx_detect_head), TRUE, FALSE)
      ) %>%
      mutate(
        link_post = stringr::str_extract(text, rgx_link_post),
        link_img = stringr::str_extract(text, rgx_link_img),
        url = stringr::str_extract(text, rgx_url),
        head = stringr::str_extract(text, rgx_head)
      ) %>%
      mutate(
        is_head = ifelse(line == 1, TRUE, is_head),
        head = ifelse(line == 1, "YAML/Introduction", head)
      )

    # NOTE: Couldn't seem to get `zoo::na.locf()` to work properly.
    lines_head <-
      lines_proc %>%
      mutate(line_head = ifelse(is_head, line, 0)) %>%
      mutate(line_head = cumsum(line_head))
    
    out <-
      lines_head %>%
      select(-head) %>%
      inner_join(
        lines_head %>%
          filter(is_head == TRUE) %>%
          select(head, line_head),
        by = c("line_head")
      ) %>% 
      select(-line_head)
    out
  }

convert_name_to_date <- function(x) {
  x %>% 
    stringr::str_extract("[0-9]{4}-[0-9]+-[0-9]+") %>% 
    strftime("%Y-%m-%d") %>% 
    lubridate::ymd()
}

data <-
  posts_info %>% 
  transmute(num_post = row_number(), name, path) %>% 
  tidyr::nest(path, .key = "path") %>% 
  mutate(data = purrr::map(path, get_rweekly_post_data)) %>% 
  select(-path) %>% 
  tidyr::unnest(data)

data <-
  data %>% 
  mutate(date = convert_name_to_date(name)) %>% 
  mutate(
    yyyy = lubridate::year(date) %>% as.integer(),
    mm = lubridate::month(date, label = TRUE),
    wd = lubridate::wday(date, label = TRUE)
  ) %>% 
  select(date, yyyy, mm, wd, everything())
data

data %>% arrange(desc(date))
```

```{r explore_time}
lab_subtitle_all <-
  paste0(
    "From ",
    strftime(data$date[1], "%Y-%m-%d"),
    " to ",
    strftime(rev(data$date)[1], "%Y-%m-%d")
  )

viz_time_all <-
  visualize_time_at(
    data = data,
    timebin = "date",
    geom = "hist",
    color = "cornflowerblue",
    lab_subtitle = lab_subtitle_all
  )
viz_time_all
```

```{r explore_1}
rgx_ignore <-
  paste0(
  "http|https|href|class|br|rweekly|",
  "en|lang|src|weekly|post|hashtag|",
  "(t.co)|image|(twitter.com)|(github.com)|",
  "hash|mdash|text|html|ltr|dir|hostname|_blank"
  )
  
unigrams <-
  data %>%
  tetext::tidify_to_unigrams_at(
    text = "text",
    rgx_ignore = rgx_ignore
  )

bigrams <-
  data %>%
  tetext::tidify_to_bigrams_at(
    text = "text",
    rgx_ignore = rgx_ignore
  )

viz_unigrams_cnts <-
  unigrams %>% 
  tetext::visualize_cnts_at(
    word = "word",
    num_top = 20
  )
viz_unigrams_cnts
viz_bigrams_cnts <-
  bigrams %>% 
  tetext::visualize_cnts_at(
    word = "word",
    num_top = 30
  )
viz_bigrams_cnts

unigram_corrs <-
  unigrams %>%
  tetext::compute_corrs_at(
    word = "word",
    feature = "name",
    num_top_ngrams = 100,
    num_top_corrs = 100
  )
unigram_corrs
unigram_corrs %>% arrange(desc(rank))

viz_unigrams_corrs <-
  unigrams %>%
  rename(feature = name) %>% 
  tetext::visualize_corrs_network(
    feature = "feature"
  )
viz_unigrams_corrs
```



